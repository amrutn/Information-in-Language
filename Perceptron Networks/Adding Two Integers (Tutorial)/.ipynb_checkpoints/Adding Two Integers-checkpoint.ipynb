{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..') #This line adds '..' to the path so we can import the net_framework python file\n",
    "from net_framework import *\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell allows you to toggle warnings on and off. It helps the readability of the results but warnings in general are important and should not be ignored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "code_show_err=false; \n",
       "function code_toggle_err() {\n",
       " if (code_show_err){\n",
       " $('div.output_stderr').hide();\n",
       " } else {\n",
       " $('div.output_stderr').show();\n",
       " }\n",
       " code_show_err = !code_show_err\n",
       "} \n",
       "$( document ).ready(code_toggle_err);\n",
       "</script>\n",
       "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hi! This notebook will serve as a tutorial to walk through the basics of this project. In this notebook, we will model the truthfulness of a class of mathematical statements that amount to the addition of two integers between -5 and 5. We need to limit the range of integers because it would take an infinite amount of compute power to fully conceptualize the addition of any two integers with no bounds. \n",
    "\n",
    "To model the information content of the chosen set of statements, we will go through the following procedure-\n",
    "\n",
    "1. Defining a \"sub-language\" to constrain the set of statements. This will require going through the process of formalizing what the set of statements you want to model are and how to interpret them. \n",
    "2. Finding an efficient representation that can be inputted into neural networks for the characters in the language and the statements you want to model. For the most part, we will be using one-hot vectors for this task (https://en.wikipedia.org/wiki/One-hot). \n",
    "3. Generating training and validation datasets for our statements. These datasets should consist of input vectors that represent legal statements in the \"sub-language\" and the associated truth values for each of these statements. \n",
    "4. Use the data to train a set of neural networks with different structures neural complexities. The ones with training error below a certain threshold will be validated. We will be using a threshold value of 0.05 for this notebok. The networks with validation error below that threshold will be classified as \"good\" networks and the \"good\" network with minimal neural complexity will be used as the final model to represent the statements. \n",
    "5. (optional) Try finding the information complexity (the amount data it takes to train the network) of that network structure by training it with different amounts of training data and seeing how much training data is required for it to have training and validation errors below the threshold. \n",
    "6. Save the statistics from the final network. This will include things such as the final neural complexity (the number of hidden layer nodes in the network), information complexity (the amount data it takes to train the network), final validation/training error, final weights of the network and plots of the training error as the number of training cycles increases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Defining a Sub-Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a purely conceptual step but is easily the most important and difficult part of this project. We need to define a language to constrain the set of statements we are working with. To do this, we need to define three things:\n",
    "\n",
    "1. The characters in the language\n",
    "2. The way these characters are legally allowed to be combined. \n",
    "3. How to determine the truthfulness of a given statement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case, the set of characters we are using can be split into two groups. \n",
    "\n",
    "1. The set of integers from -5 to 5, inclusive. We will call this set, $Z$.\n",
    "2. The symbols '+' and '='.\n",
    "\n",
    "These characters can only legally be combined in a single way.\n",
    "\n",
    "Let $z_1$, $z_2$ and $z_3$ be three elements of $Z$. Then, we define a legal 'sentence' in this language as the following combination of characters: \n",
    "\n",
    "$$z_1 + z_2 = z_3$$.\n",
    "\n",
    "And the veracity of these statements can be understood in the normal way, the statement is true if and only if the first two integers add up to the third. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it might seem unecessary to model the '+' and '=' symbols since they are in the same positions every time, but this is just a very simple example and modeling these types of symbols will be necessary when dealing with more complex syntaxes in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Finding a Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to represent our language in a way that can be understood by the neural network. To do this, we will be using one-hot vectors (https://en.wikipedia.org/wiki/One-hot) to represent each character in the network. Each vector will have a dimensionality of 13 to represent each of the 11 integers as well the '+' and '=' symbols. \n",
    "\n",
    "We will use the following mapping: \n",
    "\n",
    "'+' maps to [1,0,0,...,0,0]  \n",
    "'=' maps to [0,1,0,...,0,0]  \n",
    "'-5' maps to [0,0,1,...,0,0]  \n",
    "...  \n",
    "'4' maps to [0,0,0,...,1,0]  \n",
    "and  \n",
    "'5' maps to [0,0,0,...,0,1]  \n",
    "\n",
    "Each sentence in the langauge can be thought of as a sequence of five of these vectors stacked end to end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generating Training/Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to write functions that will generate training or validation datasets on command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(symbols):\n",
    "    '''\n",
    "    Converts symbol ('+', '=', int. between -5 to 5) array into 'stacked' one-hot vectors as specified above. \n",
    "    '''\n",
    "    vector_stack = []\n",
    "    for symbol in symbols:\n",
    "        vector = np.zeros(16)\n",
    "        if symbol == '+':\n",
    "            vector[0] = 1\n",
    "        elif symbol == '-':\n",
    "            vector[1] = 1\n",
    "        elif symbol == '*':\n",
    "            vector[2] = 1\n",
    "        elif symbol == '/':\n",
    "            vector[3] = 1\n",
    "        elif symbol =='=':\n",
    "            vector[4] = 1\n",
    "        else:\n",
    "            idx = int(symbol) + 10\n",
    "            vector[idx] = 1\n",
    "        vector_stack = np.concatenate((vector_stack, vector))\n",
    "    return np.asarray(vector_stack)\n",
    "    \n",
    "    \n",
    "def gen_data(num_examples, randomize = True):\n",
    "    \n",
    "    '''\n",
    "    Generates statements in this language as well as their veracity.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    num_examples : int\n",
    "        The number of examples in the dataset. \n",
    "    randomize : bool\n",
    "        Whether or not to randomize the output dataset. If\n",
    "        False, returns the entire possible dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X : 2D numpy array\n",
    "        Matrix of inputs where each row is a vector that represents a single sentence.\n",
    "    Y : 1D numpy array\n",
    "        Each element 1 if the corresponding statement in X is true and 0 otherwise. \n",
    "    '''\n",
    "    X = []\n",
    "    Y = []\n",
    "    sentences = []\n",
    "    i = 0\n",
    "    operations = ['+','-','*','/']\n",
    "    if randomize:\n",
    "        while i < num_examples:\n",
    "            if i < num_examples/2:\n",
    "                #Randomly Choosing three integers between -5 and 5\n",
    "                z = np.random.randint(-5, 5 + 1, 3)\n",
    "                operations = ['+','-','*','/']\n",
    "                opNum = operations[np.random.randint(0,4)]\n",
    "                sentence = np.array([z[0], operations[opNum], z[1], '=', z[2]])\n",
    "                sentences.append(sentence)\n",
    "                X.append(one_hot(sentence))\n",
    "                #switch statement for each case because i'm dumb and don't see another way \n",
    "                #to do this\n",
    "                if opNum is 0:\n",
    "                    if z[0] + z[1] == z[2]:\n",
    "                        Y.append(1)\n",
    "                if opNum is 1:\n",
    "                    if z[0] - z[1] == z[2]:\n",
    "                        Y.append(1)\n",
    "                if opNum is 2:\n",
    "                    if z[0] * z[1] == z[2]:\n",
    "                        Y.append(1)\n",
    "                if opNum is 3:\n",
    "                    if z[1] == 0:\n",
    "                        Y.append(0)\n",
    "                    elif z[0] / z[1] == z[2]:\n",
    "                        Y.append(1)\n",
    "                        \n",
    "                else:\n",
    "                    Y.append(0)\n",
    "            else:\n",
    "                #Choosing values such that the output is true to ensure that our training dataset\n",
    "                #is not skewed with False results.\n",
    "                z = np.random.randint(-5, 6, 2)\n",
    "                opNum = operations[np.random.randint(0,4)]\n",
    "                if opNum is 0:\n",
    "                    z = np.append(z, z[0] + z[1])\n",
    "                if opNum is 1:\n",
    "                    z = np.append(z, z[0] - z[1])\n",
    "                if opNum is 2:\n",
    "                    z = np.append(z, z[0] * z[1])\n",
    "                if opNum is 3:\n",
    "                    while z[1] is 0:\n",
    "                        z[1] = np.random.randint(-5, 6)\n",
    "                    z = np.append(z, z[0] / z[1])\n",
    "\n",
    "                sentence = np.array([z[0], operations[opNum], z[1], '=', z[2]])\n",
    "                sentences.append(sentence)\n",
    "                X.append(one_hot(sentence))\n",
    "                #Always correct, safeguard removed\n",
    "                Y.append(1)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        return np.asarray(X), np.asarray(Y).reshape(len(Y), 1), np.asarray(sentences)\n",
    "    else:\n",
    "        for i in np.arange(-5, 6):\n",
    "            for j in np.arange(-5, 6):\n",
    "                for k in np.arange(-5, 6):\n",
    "                    for o in operations:\n",
    "                        z = [i,j,k]\n",
    "                        sentence = np.array([z[0], o, z[1], '=', z[2]])\n",
    "                        sentences.append(sentence)\n",
    "                        X.append(one_hot(sentence))\n",
    "                        #Checking for veracity\n",
    "                        if o is '+':\n",
    "                            if z[0] + z[1] == z[2]:\n",
    "                                Y.append(1)\n",
    "                            else:\n",
    "                                Y.append(0)\n",
    "                        elif o is '-':\n",
    "                            if z[0] - z[1] == z[2]:\n",
    "                                Y.append(1)\n",
    "                            else:\n",
    "                                Y.append(0)\n",
    "                        elif o is '*':\n",
    "                            if z[0] * z[1] == z[2]:\n",
    "                                Y.append(1)\n",
    "                            else:\n",
    "                                Y.append(0)\n",
    "                        elif o is '/':\n",
    "                            if z[1] == 0:\n",
    "                                Y.append(0)\n",
    "                            elif z[0] / z[1] == z[2]:\n",
    "                                Y.append(1)\n",
    "                            else:\n",
    "                                Y.append(0)\n",
    "                        else:\n",
    "                            Y.append(0)\n",
    "        return np.asarray(X), np.asarray(Y).reshape(len(Y), 1), np.asarray(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.],\n",
      "       [0., 0., 0., ..., 0., 0., 1.]]), array([[0],\n",
      "       [0],\n",
      "       [0],\n",
      "       ...,\n",
      "       [0],\n",
      "       [0],\n",
      "       [0]]), array([['-5', '+', '-5', '=', '-5'],\n",
      "       ['-5', '-', '-5', '=', '-5'],\n",
      "       ['-5', '*', '-5', '=', '-5'],\n",
      "       ...,\n",
      "       ['5', '-', '5', '=', '5'],\n",
      "       ['5', '*', '5', '=', '5'],\n",
      "       ['5', '/', '5', '=', '5']], dtype='<U11'))\n"
     ]
    }
   ],
   "source": [
    "data = gen_data(1, randomize = False)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize our training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A typical sentence is: ['-5' '/' '-3' '=' '-2']\n",
      "The truth value of this sentence is: [0]\n",
      "Its one-hot representation is: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train = data[0]\n",
    "Y_train = data[1]\n",
    "sentences = data[2]\n",
    "print('A typical sentence is: ' + str(sentences[103]))\n",
    "print('The truth value of this sentence is: ' + str(Y_train[103]))\n",
    "print('Its one-hot representation is: ' + str(X_train[103]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training and Validating the Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to define and train a bunch of different neural networks of various shapes. We will train over 100 iterations with the entire dataset of $11^3 = 1331$ training samples. \n",
    "\n",
    "The neural networks we are using will have an input size of 5 * 13 = 65 since that is the size of 5 one-hot vectors stacked on each other. The output size will be 1. We will be trying networks with one hidden layer and varying sizes of 20, 10 and 5, networks with two hidden layers with sizes of [10,5] and [5,3]. We will also be using a learning rate of 10 but other learning rates can work as well. \n",
    "\n",
    "In this case, we are creating a binary network so we take the sigmoid of the output and round network values to 1 if they are greater than 0.5 and 0 otherwise. We want network which makes 0 errors which means it has learned the entire system. The loss function we are using during training is the binary cross entropy loss (don't worry about it for now), and the training error we are printing is the l1 norm, or the function $|y_{true} - y_{pred}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape:\n",
      "Input Size = 80\n",
      "Hidden Size = [5]\n",
      "Output Size = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#199 Error: 0.040335044264793396'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape:\n",
      "Input Size = 80\n",
      "Hidden Size = [4]\n",
      "Output Size = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#12 Error: 0.0677976906299591'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4f9866424370>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mdh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Error: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[1;31m#Saves the training results in a filed named \"Net 0\", \"Net 1\", etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveWeights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"saved_networks/Net \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\GitRepo\\Information-in-Language\\Perceptron Networks\\net_framework.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Does the update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \"\"\"\n\u001b[1;32m--> 118\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Number of training iterations\n",
    "num_iters = 200\n",
    "#Size of training dataset\n",
    "num_examples = 1331\n",
    "#Listing out the shapes of each model\n",
    "network_shapes = [(80, [5], 1), (80, [4], 1), (80, [3], 1), (80, [2], 1), (80, [2, 1], 1)]\n",
    "#Learning rate of the network\n",
    "rate = 7\n",
    "#Generating Training Data\n",
    "training_data = gen_data(num_examples, randomize = False)\n",
    "X = torch.from_numpy(training_data[0]).float()\n",
    "Y = torch.from_numpy(training_data[1]).float()\n",
    "\n",
    "#Array of losses over training period for each network\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print('Network Shape:',flush = True)\n",
    "    print('Input Size = ' + str(shape[0]), flush = True)\n",
    "    print('Hidden Size = ' + str(shape[1]), flush = True)\n",
    "    print('Output Size = ' + str(shape[2]), flush = True)\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                        hiddenSize = shape[1] , learning_rate = rate)\n",
    "    \n",
    "\n",
    "    for i in range(num_iters):\n",
    "        #Calculating l1 error\n",
    "        error = NN.l1error(Y, NN(X))\n",
    "        if i == 0: \n",
    "            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n",
    "        else:\n",
    "            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n",
    "            \n",
    "        NN.train(X, Y)\n",
    "    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n",
    "    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the network with 3 nodes is the smallest that was able to get to 0.0 error. However, the one with 5 nodes was not which suggests the optimization process is imprecise and, in general, will have to be run multiple times to get a good error. If you rerun the above cells, you might not get the same results I did. \n",
    "\n",
    "To confirm that our network works, we should testits output for various indices and ensure that it is correct. Here, we will do so for 20 randomly picked indices, at least 10 of which have a true output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network Shape:\n",
      "Input Size = 80\n",
      "Hidden Size = [5]\n",
      "Output Size = 1\n",
      "The validation error is: 0.027610819786787033\n",
      "Network Shape:\n",
      "Input Size = 80\n",
      "Hidden Size = [4]\n",
      "Output Size = 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "saved_networks/Net 1 is a zip archive (did you mean to use torch.jit.load()?)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    186\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ascii\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"strict\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m\"0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 8: '_v2\\nq\\x03(('",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2288\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2289\u001b[1;33m                 \u001b[0mtarinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarinfo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromtarfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2290\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEOFHeaderError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mfromtarfile\u001b[1;34m(cls, tarfile)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBLOCKSIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1095\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrombuf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1096\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mBLOCKSIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mfrombuf\u001b[1;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[0;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1037\u001b[1;33m         \u001b[0mchksum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnti\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m148\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m156\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1038\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mchksum\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcalc_chksums\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mnti\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInvalidHeaderError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"invalid header\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidHeaderError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mlegacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTarError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mlegacy_load\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPAX_FORMAT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m                 \u001b[0mmkdtemp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtmpdir\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1590\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCompressionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unknown compression type %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcomptype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1591\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mtaropen\u001b[1;34m(cls, name, mode, fileobj, **kwargs)\u001b[0m\n\u001b[0;32m   1620\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"mode must be 'r', 'a', 'w' or 'x'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1621\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1622\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[0;32m   1483\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirstmember\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\tarfile.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2300\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2301\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mReadError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2302\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mEmptyHeaderError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mReadError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b66eba12718c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n\u001b[0;32m     18\u001b[0m                         hiddenSize = shape[1] , learning_rate = rate)\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"saved_networks/Net \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mvalidation_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml1error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ethan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    557\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m                 \u001b[1;31m# .zip is used for torch.jit.save and will throw an un-pickling error here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{} is a zip archive (did you mean to use torch.jit.load()?)\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m             \u001b[1;31m# if not a tarfile, reset file offset and proceed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: saved_networks/Net 1 is a zip archive (did you mean to use torch.jit.load()?)"
     ]
    }
   ],
   "source": [
    "num_examples = 1331\n",
    "#Generating validation data\n",
    "validation_data = gen_data(num_examples, randomize = False)\n",
    "X = validation_data[0]\n",
    "Y = validation_data[1]\n",
    "#Converting validation data into pytorch format\n",
    "X = torch.from_numpy(X).float()\n",
    "Y = torch.from_numpy(Y).float()\n",
    "\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print('Network Shape:',flush = True)\n",
    "    print('Input Size = ' + str(shape[0]), flush = True)\n",
    "    print('Hidden Size = ' + str(shape[1]), flush = True)\n",
    "    print('Output Size = ' + str(shape[2]), flush = True)\n",
    "    \n",
    "    #Loading the network we trained in the prev. section\n",
    "    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                        hiddenSize = shape[1] , learning_rate = rate)\n",
    "    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n",
    "    validation_error = NN.l1error(Y, torch.round(NN(X)))\n",
    "\n",
    "    print(\"The validation error is: \" + str(validation_error), flush = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our minimum complexity network does seem to predict each truth value correctly. This is a good confirmation step to make sure the network is working properly and there is no error in your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Save and Discuss Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networks in this notebook are saved in the saved_networks folder. It is important to save these since sometimes training can take a long time and might not produce your desired result every time. \n",
    "\n",
    "We see from these results that our minimal complexity network in this case has three nodes. This makes logical sense since it essentially equates to one node per integer being chosen. Next, it would be useful to incorporate other operations such as subtraction, multiplication and division."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "438px",
    "left": "1306px",
    "right": "20px",
    "top": "96px",
    "width": "600px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
