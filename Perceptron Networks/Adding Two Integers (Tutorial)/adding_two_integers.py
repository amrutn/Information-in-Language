# -*- coding: utf-8 -*-
"""Adding Two Integers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rmZVJgOCI496uwdo49Jjo8DoioEByKTh
"""

import os
import sys
sys.path.insert(0, '..') #This line adds '..' to the path so we can import the net_framework python file
from net_framework import *
import numpy as np
from IPython.display import clear_output, display
import matplotlib.pyplot as plt
import scipy.stats as sp

py_file_location = "/content/Information-in-Language/Perceptron Networks"
sys.path.append(os.path .abspath(py_file_location))
from net_framework import *

import torch
from torch import nn, optim
import torch.nn.functional as F
import numpy as np

"""The following cell allows you to toggle warnings on and off. It helps the readability of the results but warnings in general are important and should not be ignored."""

def one_hot(symbols):
    '''
    Converts symbol ('+', '=', int. between -50 to 50) array into 'stacked' one-hot vectors as specified above. 
    '''
    vector_stack = []
    for symbol in symbols:
        vector = np.zeros(13)
        if symbol == '+':
            vector[0] = 1
        elif symbol == '=':
            vector[1] = 1
        else:
            idx = int(symbol) + 7
            vector[idx] = 1
        vector_stack = np.concatenate((vector_stack, vector))
    return np.asarray(vector_stack)
    
    
def gen_data(num_examples, randomize = True):
    
    '''
    Generates statements in this language as well as their veracity.
    
    Params
    ------
    num_examples : int
        The number of examples in the dataset. 
    randomize : bool
        Whether or not to randomize the output dataset. If
        False, returns the entire possible dataset.
        
    Returns
    -------
    X : 2D numpy array
        Matrix of inputs where each row is a vector that represents a single sentence.
    Y : 1D numpy array
        Each element 1 if the corresponding statement in X is true and 0 otherwise. 
    '''
    X = []
    Y = []
    sentences = []
    i = 0
    
    if randomize:
        while i < num_examples:
            if i < num_examples/2:
                #Randomly Choosing three integers between -50 and 50
                z = np.random.randint(-5, 5 + 1, 3)
                sentence = np.array([z[0], '+', z[1], '=', z[2]])
                sentences.append(sentence)
                X.append(one_hot(sentence))
                if z[0] + z[1] == z[2]:
                    Y.append(1)
                else:
                    Y.append(0)
            else:
                #Choosing values such that the output is true to ensure that our training dataset
                #is not skewed with False results.
                z = np.random.randint(-5, 6, 2)
                z = np.append(z, z[0] + z[1])
                #Ensuring the sentence is legal
                if z[2] in np.arange(-5, 6, 1):
                    sentence = np.array([z[0], '+', z[1], '=', z[2]])
                    sentences.append(sentence)
                    X.append(one_hot(sentence))
                    if z[0] + z[1] == z[2]:
                        Y.append(1)
                    else:
                        Y.append(0)
                else:
                    i -= 1
            i += 1

        return np.asarray(X), np.asarray(Y).reshape(len(Y), 1), np.asarray(sentences)
    else:
        for i in np.arange(-5, 6):
            for j in np.arange(-5, 6):
                for k in np.arange(-5, 6):
                    z = [i,j,k]
                    sentence = np.array([z[0], '+', z[1], '=', z[2]])
                    sentences.append(sentence)
                    X.append(one_hot(sentence))
                    if z[0] + z[1] == z[2]:
                        Y.append(1)
                    else:
                        Y.append(0)
        return np.asarray(X), np.asarray(Y).reshape(len(Y), 1), np.asarray(sentences)

data = gen_data(0, randomize = False)

"""We can now visualize our training data"""

X_train = data[0]
Y_train = data[1]
sentences = data[2]
print('A typical sentence is: ' + str(sentences[103]))
print('The truth value of this sentence is: ' + str(Y_train[103]))
print('Its one-hot representation is: ' + str(X_train[103]))

"""## Step 4: Training and Validating the Networks

Now, we need to define and train a bunch of different neural networks of various shapes. We will train over 100 iterations with the entire dataset of $11^3 = 1331$ training samples. 

The neural networks we are using will have an input size of 5 * 13 = 65 since that is the size of 5 one-hot vectors stacked on each other. The output size will be 1. We will be trying networks with one hidden layer and varying sizes of 20, 10 and 5, networks with two hidden layers with sizes of [10,5] and [5,3]. We will also be using a learning rate of 10 but other learning rates can work as well. 

In this case, we are creating a binary network so we take the sigmoid of the output and round network values to 1 if they are greater than 0.5 and 0 otherwise. We want network which makes 0 errors which means it has learned the entire system. The loss function we are using during training is the binary cross entropy loss (don't worry about it for now), and the training error we are printing is the l1 norm, or the function $|y_{true} - y_{pred}|$
"""

# #Move model to GPU
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# print('device', device)

# #Instantiate model with 6 classes
# model = UNet(6) #CODE_HERE
# model = model.to(device)

"""### Training"""

#Number of training iterations
num_iters = 200
#Size of training dataset
num_examples = 1331
#Listing out the shapes of each model
network_shapes = [(65, [5], 1), (65, [4], 1), (65, [3], 1), (65, [2], 1), (65, [2, 1], 1)]
#Learning rate of the network
rate = 7
#Generating Training Data
training_data = gen_data(num_examples, randomize = False)
X = torch.from_numpy(training_data[0]).float()
Y = torch.from_numpy(training_data[1]).float()

#Array of losses over training period for each network
for net_num, shape in enumerate(network_shapes):
    print('Network Shape:',flush = True)
    print('Input Size = ' + str(shape[0]), flush = True)
    print('Hidden Size = ' + str(shape[1]), flush = True)
    print('Output Size = ' + str(shape[2]), flush = True)
    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],
                        hiddenSize = shape[1] , learning_rate = rate)
    

    for i in range(num_iters):
        #Calculating l1 error
        error = NN.l1error(Y, NN(X))
        if i == 0: 
            dh = display("#" + str(i) + " Error: " + str(error), display_id=True)
        else:
            dh.update("#" + str(i) + " Error: " + str(error))
            
        NN.train(X, Y)
    #Saves the training results in a filed named "Net 0", "Net 1", etc. 
    NN.saveWeights(model = NN, path = "saved_networks/Net " + str(net_num));

"""### Validation

In this step, we will see how well the networks did at actually guessing the truth values of the statements and whether or not any of them got near 0 error. 0 error is possible if we round all network outputs to 0 or 1.
"""

num_examples = 1331
#Generating validation data
validation_data = gen_data(num_examples, randomize = False)
X = validation_data[0]
Y = validation_data[1]
#Converting validation data into pytorch format
X = torch.from_numpy(X).float()
Y = torch.from_numpy(Y).float()

for net_num, shape in enumerate(network_shapes):
    print('Network Shape:',flush = True)
    print('Input Size = ' + str(shape[0]), flush = True)
    print('Hidden Size = ' + str(shape[1]), flush = True)
    print('Output Size = ' + str(shape[2]), flush = True)
    
    #Loading the network we trained in the prev. section
    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],
                        hiddenSize = shape[1] , learning_rate = rate)
    NN.load_state_dict(torch.load("Information-in-Language/Perceptron Networks/Adding Two Integers (Tutorial)/saved_networks/Net " + str(net_num)))
    validation_error = NN.l1error(Y, torch.round(NN(X)))

    print("The validation error is: " + str(validation_error), flush = True)

"""We see that the network with 3 nodes is the smallest that was able to get to 0.0 error. However, the one with 5 nodes was not which suggests the optimization process is imprecise and, in general, will have to be run multiple times to get a good error. If you rerun the above cells, you might not get the same results I did. 

To confirm that our network works, we should testits output for various indices and ensure that it is correct. Here, we will do so for 20 randomly picked indices, at least 10 of which have a true output
"""

indices_true = np.argwhere(Y.T[0] == 1)[0]
random_true_indices = np.random.choice(indices_true, size = 10)
indices = np.random.random_integers(0, 1331, 10)
indices = np.append(indices, random_true_indices)
#The sentences we are modelling from the previous dataset.
sentences = validation_data[2]
#Getting the best network from the previous validation set
NN = Neural_Network(inputSize = 65, outputSize = 1,
                        hiddenSize = [3] , learning_rate = 7)
NN.load_state_dict(torch.load("Information-in-Language/Perceptron Networks/Adding Two Integers (Tutorial)/saved_networks/Net 2"))
#Predicting values for the dataset
predicted = NN(X, binary = True)
for index in indices:
    print("The statement is: " + str(sentences[index]))
    print("Its truth value is: " + str(predicted[index].item()))

"""We see that our minimum complexity network does seem to predict each truth value correctly. This is a good confirmation step to make sure the network is working properly and there is no error in your code.

## Step 6: Save and Discuss Results

The networks in this notebook are saved in the saved_networks folder. It is important to save these since sometimes training can take a long time and might not produce your desired result every time. 

We see from these results that our minimal complexity network in this case has three nodes. This makes logical sense since it essentially equates to one node per integer being chosen. Next, it would be useful to incorporate other operations such as subtraction, multiplication and division.
"""