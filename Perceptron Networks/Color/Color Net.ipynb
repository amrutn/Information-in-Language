{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "sys.path.insert(0, '..')\n",
    "from net_framework import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Lnum</th>\n",
       "      <th>#snum</th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Term Abbrev</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>LB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>LE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>WK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>LF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #Lnum  #snum  #cnum Term Abbrev\n",
       "0      1      1      1          LB\n",
       "1      1      1      2          LB\n",
       "2      1      1      3          LE\n",
       "3      1      1      4          WK\n",
       "4      1      1      5          LF"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\n",
    "term_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\n",
    "term_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "        14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,\n",
       "        27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,\n",
       "        40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,\n",
       "        53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,  65,\n",
       "        66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,\n",
       "        79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,\n",
       "        92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103, 104,\n",
       "       105, 106, 107, 108, 109, 110], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_data['#Lnum'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#cnum</th>\n",
       "      <th>Normalized-L</th>\n",
       "      <th>Normalized-a</th>\n",
       "      <th>Normalized-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141</td>\n",
       "      <td>0.853205</td>\n",
       "      <td>-0.025675</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>274</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>-0.025504</td>\n",
       "      <td>-0.138822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>129</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070445</td>\n",
       "      <td>-0.106039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070101</td>\n",
       "      <td>-0.089951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>302</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.070617</td>\n",
       "      <td>-0.072041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>305</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.567556</td>\n",
       "      <td>-0.362691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>267</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.584752</td>\n",
       "      <td>-0.297579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>243</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.593865</td>\n",
       "      <td>-0.235807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>182</td>\n",
       "      <td>-0.765518</td>\n",
       "      <td>0.601603</td>\n",
       "      <td>-0.171302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>89</td>\n",
       "      <td>-0.871488</td>\n",
       "      <td>-0.024988</td>\n",
       "      <td>-0.139429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     #cnum  Normalized-L  Normalized-a  Normalized-b\n",
       "0      141      0.853205     -0.025675     -0.138822\n",
       "1      274      0.747664     -0.025504     -0.138822\n",
       "2      129      0.747664      0.070445     -0.106039\n",
       "3      230      0.747664      0.070101     -0.089951\n",
       "4      302      0.747664      0.070617     -0.072041\n",
       "..     ...           ...           ...           ...\n",
       "325    305     -0.765518      0.567556     -0.362691\n",
       "326    267     -0.765518      0.584752     -0.297579\n",
       "327    243     -0.765518      0.593865     -0.235807\n",
       "328    182     -0.765518      0.601603     -0.171302\n",
       "329     89     -0.871488     -0.024988     -0.139429\n",
       "\n",
       "[330 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\n",
    "locations = cnum_data[['#cnum']]\n",
    "locations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).std() * 1/2\n",
    "locations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\n",
    "locations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\n",
    "display(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n",
    "# debug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n",
    "# debug1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations = locations.sort_values('#cnum')\n",
    "chip_num = list(locations['#cnum'])\n",
    "lab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]\n",
    "# print(lab_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Neural Network Shape Test\n",
    "# NNtest = Neural_Network(inputSize = 3, outputSize = 9, hiddenSize = [3,3,3] , learning_rate = 0.001)\n",
    "# NNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_num=5\n",
    "l1 = term_data[term_data.get('#Lnum').eq(language_num)]\n",
    "unique_symbols = list(l1['Term Abbrev'].unique())\n",
    "l1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n",
    "                              for abbrev in unique_symbols] for i in range(len(l1_grouped))]\n",
    "#display(l1_chip_abbrev_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              7         2    4         6         9         1         5    3  \\\n",
      "#cnum                                                                         \n",
      "1      0.833333  0.166667  0.0  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "2      0.333333  0.666667  0.0  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "3      0.000000  0.000000  1.0  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "4      0.000000  0.000000  0.0  1.000000  0.000000  0.000000  0.000000  0.0   \n",
      "5      0.333333  0.666667  0.0  0.000000  0.000000  0.000000  0.000000  0.0   \n",
      "...         ...       ...  ...       ...       ...       ...       ...  ...   \n",
      "326    0.000000  0.000000  0.0  0.000000  0.000000  0.000000  1.000000  0.0   \n",
      "327    0.166667  0.000000  0.0  0.000000  0.666667  0.000000  0.166667  0.0   \n",
      "328    0.166667  0.666667  0.0  0.166667  0.000000  0.000000  0.000000  0.0   \n",
      "329    0.000000  0.000000  0.0  0.333333  0.000000  0.166667  0.500000  0.0   \n",
      "330    0.166667  0.000000  0.0  0.000000  0.833333  0.000000  0.000000  0.0   \n",
      "\n",
      "         8  \n",
      "#cnum       \n",
      "1      0.0  \n",
      "2      0.0  \n",
      "3      0.0  \n",
      "4      0.0  \n",
      "5      0.0  \n",
      "...    ...  \n",
      "326    0.0  \n",
      "327    0.0  \n",
      "328    0.0  \n",
      "329    0.0  \n",
      "330    0.0  \n",
      "\n",
      "[330 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "l1_result.index += 1\n",
    "l1_result.index.name = '#cnum'\n",
    "l1_result.columns = unique_symbols\n",
    "print(l1_result)\n",
    "chip_norm = []\n",
    "#pull the percentage for each cnum\n",
    "for x in chip_num:\n",
    "#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n",
    "    chip_norm.append(l1_result.loc[x,:].values.tolist())\n",
    "#display(chip_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Network Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3], [4], [5], [6], [3, 3], [7], [8], [4, 4], [9], [3, 3, 3], [10], [5, 5], [3, 4, 3], [11], [3, 5, 3], [12], [6, 6], [4, 4, 4], [13], [4, 5, 4], [14], [7, 7], [4, 6, 4], [15], [5, 5, 5], [16], [8, 8], [5, 6, 5], [17], [5, 7, 5], [18], [9, 9], [6, 6, 6], [19], [6, 7, 6], [20], [10, 10], [6, 8, 6], [21], [7, 7, 7], [22], [11, 11], [7, 8, 7], [23], [7, 9, 7], [24], [12, 12], [8, 8, 8]]\n"
     ]
    }
   ],
   "source": [
    "node_num = range(1,25)\n",
    "layer_num = range(1,4)\n",
    "\n",
    "\n",
    "shape_collection = []\n",
    "for node in node_num:\n",
    "    if node < 3:\n",
    "        shape_collection.append([node])\n",
    "\n",
    "def trickle(arr, iteration_left, check):\n",
    "    if iteration_left == 0:\n",
    "        global shape_collection\n",
    "        #running the int fxn to make sure we don't have floats\n",
    "        mp = map(int, arr)\n",
    "        x = list(mp)\n",
    "        if check == sum(x):\n",
    "            shape_collection.append(x)\n",
    "    else:\n",
    "        new_arr = [0]+ arr + [0]\n",
    "        #recursively expanding the list symmetrically\n",
    "        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n",
    "            new_arr[0] += 1\n",
    "            new_arr[1] -= 1\n",
    "            new_arr[-1] += 1\n",
    "            new_arr[-2] -= 1\n",
    "        trickle(new_arr, iteration_left - 1, check)\n",
    "\n",
    "for node in node_num:\n",
    "    for layer in layer_num:\n",
    "        if node//layer < 3:\n",
    "            continue\n",
    "        if layer%2 == 0:\n",
    "            trickle([node/2, node/2], (layer-2)/2, node)\n",
    "        else:\n",
    "            trickle([node], (layer-1)/2, node)      \n",
    "\n",
    "print(shape_collection)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, [1], 9), (3, [2], 9), (3, [3], 9), (3, [4], 9), (3, [5], 9), (3, [6], 9), (3, [3, 3], 9), (3, [7], 9), (3, [8], 9), (3, [4, 4], 9), (3, [9], 9), (3, [3, 3, 3], 9), (3, [10], 9), (3, [5, 5], 9), (3, [3, 4, 3], 9), (3, [11], 9), (3, [3, 5, 3], 9), (3, [12], 9), (3, [6, 6], 9), (3, [4, 4, 4], 9), (3, [13], 9), (3, [4, 5, 4], 9), (3, [14], 9), (3, [7, 7], 9), (3, [4, 6, 4], 9), (3, [15], 9), (3, [5, 5, 5], 9), (3, [16], 9), (3, [8, 8], 9), (3, [5, 6, 5], 9), (3, [17], 9), (3, [5, 7, 5], 9), (3, [18], 9), (3, [9, 9], 9), (3, [6, 6, 6], 9), (3, [19], 9), (3, [6, 7, 6], 9), (3, [20], 9), (3, [10, 10], 9), (3, [6, 8, 6], 9), (3, [21], 9), (3, [7, 7, 7], 9), (3, [22], 9), (3, [11, 11], 9), (3, [7, 8, 7], 9), (3, [23], 9), (3, [7, 9, 7], 9), (3, [24], 9), (3, [12, 12], 9), (3, [8, 8, 8], 9)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Number of training iterations\n",
    "num_iters = 1000\n",
    "\n",
    "#Listing out the shapes of each model\n",
    "colors_num = len(chip_norm[0])\n",
    "input_size = 3\n",
    "\n",
    "network_shapes = []\n",
    "for s in shape_collection:\n",
    "    network_shapes.append((input_size,s,colors_num))\n",
    "\n",
    "#Learning rate of the network\n",
    "rate = 0.001\n",
    "\n",
    "#Generating Training Data\n",
    "def shuffle(lab_norm, chip_norm):\n",
    "    '''\n",
    "    Applying train-test split\n",
    "    '''\n",
    "    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, shuffle = True)\n",
    "    input_train = torch.FloatTensor(lab_train)\n",
    "    output_train = torch.FloatTensor(chip_train)\n",
    "    input_test= torch.FloatTensor(lab_test)\n",
    "    output_test = torch.FloatTensor(chip_test)\n",
    "    return input_train, output_train, input_test, output_test\n",
    "\n",
    "print(network_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Array of losses over training period for each network\n",
    "num_average = 10\n",
    "output_file = {}\n",
    "for n in node_num:\n",
    "    output_file[n] = {}\n",
    "    \n",
    "\n",
    "for net_num, shape in enumerate(network_shapes):\n",
    "    print(\"Training: \",shape)\n",
    "    net_error_arr = []\n",
    "    for j in range(num_average):\n",
    "        print('Run ' + str(j+1))\n",
    "        NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n",
    "                            hiddenSize = shape[1] , learning_rate = rate)\n",
    "        error_arr = []\n",
    "        prev_error = 0\n",
    "        strike = 0\n",
    "\n",
    "        input_train, output_train, input_test, output_test = shuffle(lab_norm, chip_norm)\n",
    "\n",
    "        for i in range(num_iters):  \n",
    "            NN.train(input_train, output_train)\n",
    "            validation_error = NN.l1error(output_test, NN(input_test))\n",
    "            #Printing error\n",
    "            if i == 0: \n",
    "                dh = display(\"#\" + str(i) + \" Validation Error: \" + str(validation_error), display_id=True)\n",
    "            else:\n",
    "                dh.update(\"#\" + str(i) + \" Validation Error: \" + str(validation_error))\n",
    "            \n",
    "            #zero small error change\n",
    "            if i == 0:\n",
    "                strike = 0\n",
    "            #adding error to array\n",
    "            error_arr.append(validation_error)\n",
    "            #waiting for number 'too small' decreases or increases in validation error before ending training\n",
    "            if (prev_error < validation_error) and i > 100:\n",
    "                if strike > 5:\n",
    "                    print(\"Complete at iteration \", i, \"\\nFinal error: \", np.min(error_arr), \"\\n\")\n",
    "                    break\n",
    "                else:\n",
    "                    strike += 1\n",
    "            prev_error = validation_error\n",
    "        net_error_arr.append(np.min(error_arr))\n",
    "    output_file[sum(shape[1])][len(shape[1])] = [np.mean(net_error_arr), np.std(net_error_arr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json', 'w') as f:\n",
    "    json.dump(output_file, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at minimum size of networks for each threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('validation_errors.json') as f:\n",
    "    output_file=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.2)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = []\n",
    "for size in node_num:\n",
    "    out_dict_for_size = output_file[str(size)]\n",
    "    vals = list(out_dict_for_size.values())\n",
    "    vals = np.array(vals)\n",
    "    vals = vals[:,0]\n",
    "    errors.append(np.min(vals))\n",
    "errors = np.array(errors)\n",
    "\n",
    "thresholds = np.arange(.001, 1, .001)\n",
    "\n",
    "min_sizes = []\n",
    "for threshold in thresholds:\n",
    "    idx = 0\n",
    "    for err in errors:\n",
    "        if err <= threshold:\n",
    "            break\n",
    "        idx += 1\n",
    "    if idx < len(node_num):\n",
    "        min_sizes.append(node_num[idx])\n",
    "    else:\n",
    "        min_sizes.append(max(node_num))\n",
    "        \n",
    "plt.title('Threshold Plot for Language {0}'.format(language_num))\n",
    "plt.plot(thresholds, min_sizes)\n",
    "plt.xlabel('Error Treshold Value')\n",
    "plt.ylabel('Minimum Network Size')\n",
    "plt.xlim(0,0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculating Complexity in bits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_complexity(lnum):\n",
    "    language_data = term_data[term_data.get('#Lnum').eq(lnum)]\n",
    "    unique_terms = list(language_data['Term Abbrev'].unique())\n",
    "    l1_grouped = language_data.groupby('#cnum')['Term Abbrev'].apply(list)\n",
    "    display(l1_grouped)\n",
    "    l1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n",
    "                              for abbrev in unique_terms] for i in range(len(l1_grouped))]\n",
    "    l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\n",
    "    l1_result.index += 1\n",
    "    l1_result.index.name = '#cnum'\n",
    "    l1_result.columns = unique_terms\n",
    "\n",
    "    chip_norm = []\n",
    "\n",
    "    for x in chip_num:\n",
    "        chip_norm.append(l1_result.loc[x,:].values.tolist())\n",
    "\n",
    "    terms = unique_terms\n",
    "    chips = list(language_data['#cnum'].unique())\n",
    "\n",
    "    complexity = 0\n",
    "    prior_m = 1 / len(chips)\n",
    "    for w in terms:\n",
    "        word_prob = 0\n",
    "        for m in chips:\n",
    "            word_prob += prior_m * l1_result.at[m, w]\n",
    "        for m in chips:\n",
    "            encoder_prob = l1_result.at[m, w]\n",
    "            if encoder_prob != 0:\n",
    "                mutual_information = prior_m * encoder_prob * np.log2(encoder_prob / word_prob)\n",
    "            complexity += mutual_information\n",
    "\n",
    "    return complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#cnum\n",
       "1      [7, 7, 7, 7, 7, 2]\n",
       "2      [2, 2, 7, 7, 2, 2]\n",
       "3      [4, 4, 4, 4, 4, 4]\n",
       "4      [6, 6, 6, 6, 6, 6]\n",
       "5      [7, 2, 7, 2, 2, 2]\n",
       "              ...        \n",
       "326    [5, 5, 5, 5, 5, 5]\n",
       "327    [7, 9, 9, 9, 5, 9]\n",
       "328    [2, 2, 2, 7, 2, 6]\n",
       "329    [1, 5, 5, 5, 6, 6]\n",
       "330    [7, 9, 9, 9, 9, 9]\n",
       "Name: Term Abbrev, Length: 330, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.803655245868127\n"
     ]
    }
   ],
   "source": [
    "result = language_complexity(language_num)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "98b63ce99ffa4f428115004fb0366917",
    "idx": 0,
    "time": "2021-02-03T02:17:02.768Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "idx": 1,
    "time": "2021-02-03T02:17:03.459Z",
    "type": "execution"
   },
   {
    "id": "98b63ce99ffa4f428115004fb0366917",
    "time": "2021-02-03T02:17:04.551Z",
    "type": "completion"
   },
   {
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "time": "2021-02-03T02:17:04.866Z",
    "type": "completion"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "idx": 2,
    "time": "2021-02-03T02:17:08.061Z",
    "type": "execution"
   },
   {
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "time": "2021-02-03T02:17:08.139Z",
    "type": "completion"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "467e3a66759c4d08a81372178f7abd45",
    "idx": 3,
    "time": "2021-02-03T02:17:11.506Z",
    "type": "execution"
   },
   {
    "id": "467e3a66759c4d08a81372178f7abd45",
    "time": "2021-02-03T02:17:11.558Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:17:11.748Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:17:11.840Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:17:11.958Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:17:12.253Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "idx": 6,
    "time": "2021-02-03T02:17:12.570Z",
    "type": "execution"
   },
   {
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "time": "2021-02-03T02:17:12.619Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "9277c562ac6843d08f0c0336d9745513",
    "idx": 7,
    "time": "2021-02-03T02:17:13.098Z",
    "type": "execution"
   },
   {
    "id": "9277c562ac6843d08f0c0336d9745513",
    "time": "2021-02-03T02:17:16.262Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:24:19.024Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:24:19.357Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:24:49.472Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:24:49.625Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:24:50.379Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:24:50.485Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:25:21.820Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:25:21.934Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:25:26.177Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:25:26.258Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:25:26.373Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:25:26.772Z",
    "type": "completion"
   },
   {
    "code": "chip_num",
    "id": "cf646372e8c149f695e18186263057f5",
    "idx": 4,
    "time": "2021-02-03T02:26:11.410Z",
    "type": "execution"
   },
   {
    "id": "cf646372e8c149f695e18186263057f5",
    "time": "2021-02-03T02:26:11.478Z",
    "type": "completion"
   },
   {
    "code": "lab_norm",
    "id": "cf646372e8c149f695e18186263057f5",
    "idx": 4,
    "time": "2021-02-03T02:26:22.729Z",
    "type": "execution"
   },
   {
    "id": "cf646372e8c149f695e18186263057f5",
    "time": "2021-02-03T02:26:22.830Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "cf646372e8c149f695e18186263057f5",
    "idx": 4,
    "time": "2021-02-03T02:26:29.288Z",
    "type": "execution"
   },
   {
    "id": "cf646372e8c149f695e18186263057f5",
    "time": "2021-02-03T02:26:29.375Z",
    "type": "completion"
   },
   {
    "code": "l1",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:28:27.321Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:28:27.377Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:19.390Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:19.445Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped[0]",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:25.010Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:25.098Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:34.880Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:34.932Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped[1]",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:45.667Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:45.710Z",
    "type": "completion"
   },
   {
    "code": "len(l1_grouped[1]",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:55.651Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:55.695Z",
    "type": "completion"
   },
   {
    "code": "len(l1_grouped[1])",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:29:58.486Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:29:58.586Z",
    "type": "completion"
   },
   {
    "code": "len(l1_grouped[2])",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:30:08.947Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:30:08.994Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:30:25.427Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:30:25.558Z",
    "type": "completion"
   },
   {
    "code": "(l1_grouped[2])",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:30:29.331Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:30:29.375Z",
    "type": "completion"
   },
   {
    "code": "(l1_grouped)",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:30:35.569Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:30:35.630Z",
    "type": "completion"
   },
   {
    "code": "(l1_grouped)",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:31:07.091Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:31:07.141Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:31:57.834Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:31:57.989Z",
    "type": "completion"
   },
   {
    "code": "unique_symbols",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:32:18.603Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:32:18.650Z",
    "type": "completion"
   },
   {
    "code": "len(unique_symbols)",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:32:31.570Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:32:31.661Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:32:44.502Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:32:44.556Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped\nl1_chip_abbrev_percentage",
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "idx": 5,
    "time": "2021-02-03T02:33:02.372Z",
    "type": "execution"
   },
   {
    "id": "11d0c357e9c0412a866ee6c42928bedd",
    "time": "2021-02-03T02:33:03.174Z",
    "type": "completion"
   },
   {
    "code": "l1_result",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:34:25.660Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:34:25.764Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[l1[\"#cnum\"]==1]",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:34:41.836Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:34:41.934Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[1:]",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:36:01.388Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:36:01.493Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[1,:]",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:36:16.371Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:36:16.426Z",
    "type": "completion"
   },
   {
    "code": "l1_result.loc[1,:].values.tolist()",
    "id": "c5f34e313dc24add925362a88193da74",
    "idx": 5,
    "time": "2021-02-03T02:37:22.747Z",
    "type": "execution"
   },
   {
    "id": "c5f34e313dc24add925362a88193da74",
    "time": "2021-02-03T02:37:22.801Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:40:06.885Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:40:06.970Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n    chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:40:09.847Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:40:10.221Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[1,:].values.tolist())\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:40:41.672Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:40:41.846Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:40:47.442Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:40:47.600Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:41:07.062Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:41:07.197Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:09.666Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:10.109Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm.shape()",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:27.119Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:27.254Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nchip_norm.shape",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:39.706Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:39.828Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nlen(chip_norm)\nchip_norm[0]",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:41:54.333Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:41:54.583Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nprint(len(chip_norm))\nchip_norm[0]",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:42:07.176Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:42:07.339Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())\n    \nprint(len(chip_norm))\nprint(len(chip_norm[0]))",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:42:30.050Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:42:30.147Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 6,
    "time": "2021-02-03T02:42:57.806Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:42:57.896Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "idx": 7,
    "time": "2021-02-03T02:42:58.790Z",
    "type": "execution"
   },
   {
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "time": "2021-02-03T02:42:58.834Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "9277c562ac6843d08f0c0336d9745513",
    "idx": 8,
    "time": "2021-02-03T02:44:08.571Z",
    "type": "execution"
   },
   {
    "id": "9277c562ac6843d08f0c0336d9745513",
    "time": "2021-02-03T02:45:02.125Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "idx": 9,
    "time": "2021-02-03T02:45:57.807Z",
    "type": "execution"
   },
   {
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "time": "2021-02-03T02:45:57.924Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T02:53:48.766Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T02:53:48.849Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T02:53:48.976Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T02:53:49.086Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "idx": 6,
    "time": "2021-02-03T02:53:49.316Z",
    "type": "execution"
   },
   {
    "id": "8cc05ec00119411f8f0b62b4fa50265b",
    "time": "2021-02-03T02:53:49.365Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "9277c562ac6843d08f0c0336d9745513",
    "idx": 7,
    "time": "2021-02-03T02:53:50.090Z",
    "type": "execution"
   },
   {
    "id": "9277c562ac6843d08f0c0336d9745513",
    "time": "2021-02-03T02:54:46.055Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "idx": 8,
    "time": "2021-02-03T02:54:48.422Z",
    "type": "execution"
   },
   {
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "time": "2021-02-03T02:54:48.523Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "idx": 8,
    "time": "2021-02-03T02:54:54.136Z",
    "type": "execution"
   },
   {
    "id": "21a85bde696a47ee98c20f205cad5cd4",
    "time": "2021-02-03T02:54:54.236Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "idx": 4,
    "time": "2021-02-03T04:04:36.310Z",
    "type": "execution"
   },
   {
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "time": "2021-02-03T04:04:46.687Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "98b63ce99ffa4f428115004fb0366917",
    "idx": 0,
    "time": "2021-02-03T04:04:53.275Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "idx": 1,
    "time": "2021-02-03T04:04:53.503Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "idx": 2,
    "time": "2021-02-03T04:04:53.670Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "467e3a66759c4d08a81372178f7abd45",
    "idx": 3,
    "time": "2021-02-03T04:04:53.858Z",
    "type": "execution"
   },
   {
    "id": "98b63ce99ffa4f428115004fb0366917",
    "time": "2021-02-03T04:04:54.984Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "idx": 4,
    "time": "2021-02-03T04:04:55.008Z",
    "type": "execution"
   },
   {
    "id": "20b0acb43c014af6866c6ee50356a0b2",
    "time": "2021-02-03T04:04:55.263Z",
    "type": "completion"
   },
   {
    "id": "6e3071fb1dde4215810e2a80d8e471a4",
    "time": "2021-02-03T04:04:55.293Z",
    "type": "completion"
   },
   {
    "id": "467e3a66759c4d08a81372178f7abd45",
    "time": "2021-02-03T04:04:55.297Z",
    "type": "completion"
   },
   {
    "id": "fd38caded7c2472d8d7f45c3be57ae56",
    "time": "2021-02-03T04:04:55.316Z",
    "type": "completion"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "467e3a66759c4d08a81372178f7abd45",
    "idx": 3,
    "time": "2021-02-03T04:06:06.303Z",
    "type": "execution"
   },
   {
    "id": "467e3a66759c4d08a81372178f7abd45",
    "time": "2021-02-03T04:06:06.357Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:11:42.418Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:11:42.514Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:11:42.981Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:11:43.076Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:11:46.972Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:11:47.117Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:11:47.514Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:11:47.630Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(3)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:12:27.648Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:12:27.955Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:12:29.357Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:12:29.517Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "idx": 4,
    "time": "2021-02-03T04:12:38.986Z",
    "type": "execution"
   },
   {
    "id": "02eaf4a67fb7453dadf7ec4de2ba2b00",
    "time": "2021-02-03T04:12:39.154Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "idx": 5,
    "time": "2021-02-03T04:12:40.407Z",
    "type": "execution"
   },
   {
    "id": "0f5a1ecf987049e68b1f180f53d014bd",
    "time": "2021-02-03T04:12:40.495Z",
    "type": "completion"
   },
   {
    "code": "term_data['#cnum']",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:38:47.778Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:38:47.976Z",
    "type": "completion"
   },
   {
    "code": "term_data['#cnum'].unique",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:38:54.487Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:38:54.529Z",
    "type": "completion"
   },
   {
    "code": "term_data['#cnum'].unique()",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:38:59.539Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:38:59.592Z",
    "type": "completion"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "466c00fff29a428ca60f9db17cd69273",
    "idx": 2,
    "time": "2021-02-03T04:39:07.612Z",
    "type": "execution"
   },
   {
    "id": "466c00fff29a428ca60f9db17cd69273",
    "time": "2021-02-03T04:39:07.672Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e0ac933829d471595492e4acc88ea34",
    "idx": 0,
    "time": "2021-02-06T18:06:22.906Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "4903bda737bf4f61a75993cf916b5233",
    "idx": 1,
    "time": "2021-02-06T18:06:23.110Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "idx": 2,
    "time": "2021-02-06T18:06:23.325Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "d9226fd6339f4307870bf39af7b51873",
    "idx": 3,
    "time": "2021-02-06T18:06:23.570Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "idx": 4,
    "time": "2021-02-06T18:06:23.889Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:06:24.430Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:06:25.289Z",
    "type": "execution"
   },
   {
    "id": "0e0ac933829d471595492e4acc88ea34",
    "time": "2021-02-06T18:06:25.313Z",
    "type": "completion"
   },
   {
    "id": "4903bda737bf4f61a75993cf916b5233",
    "time": "2021-02-06T18:06:25.820Z",
    "type": "completion"
   },
   {
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "time": "2021-02-06T18:06:25.853Z",
    "type": "completion"
   },
   {
    "id": "d9226fd6339f4307870bf39af7b51873",
    "time": "2021-02-06T18:06:25.889Z",
    "type": "completion"
   },
   {
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "time": "2021-02-06T18:06:25.908Z",
    "type": "completion"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:06:25.969Z",
    "type": "completion"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:06:26.066Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:06:27.543Z",
    "type": "execution"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:06:27.594Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:11:49.708Z",
    "type": "execution"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:11:49.815Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:11:49.997Z",
    "type": "execution"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:11:50.122Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:11:54.897Z",
    "type": "execution"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:11:54.948Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:12:17.336Z",
    "type": "execution"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:13:38.420Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:14:33.150Z",
    "type": "execution"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:14:33.388Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:15:52.597Z",
    "type": "execution"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:17:12.579Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:17:15.471Z",
    "type": "execution"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:17:15.790Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e0ac933829d471595492e4acc88ea34",
    "idx": 0,
    "time": "2021-02-06T18:21:50.043Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "4903bda737bf4f61a75993cf916b5233",
    "idx": 1,
    "time": "2021-02-06T18:21:50.046Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "idx": 2,
    "time": "2021-02-06T18:21:50.047Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "d9226fd6339f4307870bf39af7b51873",
    "idx": 3,
    "time": "2021-02-06T18:21:50.048Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "idx": 4,
    "time": "2021-02-06T18:21:50.049Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:21:50.050Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:21:50.051Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:21:50.052Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:21:50.053Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:21:50.054Z",
    "type": "execution"
   },
   {
    "id": "0e0ac933829d471595492e4acc88ea34",
    "time": "2021-02-06T18:21:52.356Z",
    "type": "completion"
   },
   {
    "id": "4903bda737bf4f61a75993cf916b5233",
    "time": "2021-02-06T18:21:52.798Z",
    "type": "completion"
   },
   {
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "time": "2021-02-06T18:21:52.820Z",
    "type": "completion"
   },
   {
    "id": "d9226fd6339f4307870bf39af7b51873",
    "time": "2021-02-06T18:21:52.867Z",
    "type": "completion"
   },
   {
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "time": "2021-02-06T18:21:52.876Z",
    "type": "completion"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:21:52.949Z",
    "type": "completion"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:21:53.049Z",
    "type": "completion"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:21:53.056Z",
    "type": "completion"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:21:53.266Z",
    "type": "completion"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:21:53.304Z",
    "type": "completion"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e0ac933829d471595492e4acc88ea34",
    "idx": 0,
    "time": "2021-02-06T18:22:16.243Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "4903bda737bf4f61a75993cf916b5233",
    "idx": 1,
    "time": "2021-02-06T18:22:16.244Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "idx": 2,
    "time": "2021-02-06T18:22:16.245Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "d9226fd6339f4307870bf39af7b51873",
    "idx": 3,
    "time": "2021-02-06T18:22:16.246Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "idx": 4,
    "time": "2021-02-06T18:22:16.248Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:22:16.248Z",
    "type": "execution"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:22:16.249Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 5000\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:22:16.250Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:22:16.251Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:22:16.252Z",
    "type": "execution"
   },
   {
    "id": "0e0ac933829d471595492e4acc88ea34",
    "time": "2021-02-06T18:22:18.731Z",
    "type": "completion"
   },
   {
    "id": "4903bda737bf4f61a75993cf916b5233",
    "time": "2021-02-06T18:22:19.173Z",
    "type": "completion"
   },
   {
    "id": "4907e05ff41d411c8ef84c6c8eed8834",
    "time": "2021-02-06T18:22:19.194Z",
    "type": "completion"
   },
   {
    "id": "d9226fd6339f4307870bf39af7b51873",
    "time": "2021-02-06T18:22:19.267Z",
    "type": "completion"
   },
   {
    "id": "964bc2f9ed444796ade9ad151d64c2d2",
    "time": "2021-02-06T18:22:19.279Z",
    "type": "completion"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:22:19.422Z",
    "type": "completion"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:22:19.500Z",
    "type": "completion"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:22:19.518Z",
    "type": "completion"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:23:01.889Z",
    "type": "completion"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:23:01.921Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 50\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [100], colors_num), (input_size, [100], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.1)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "44941f4587c5419d9f229a7472ac6520",
    "idx": 7,
    "time": "2021-02-06T18:23:21.229Z",
    "type": "execution"
   },
   {
    "id": "44941f4587c5419d9f229a7472ac6520",
    "time": "2021-02-06T18:23:21.285Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_train, NN(input_train))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "idx": 8,
    "time": "2021-02-06T18:23:21.456Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "idx": 9,
    "time": "2021-02-06T18:23:21.747Z",
    "type": "execution"
   },
   {
    "id": "f673e22a10ec4607805bafd68cb56ec6",
    "time": "2021-02-06T18:24:53.493Z",
    "type": "completion"
   },
   {
    "id": "5c2f3a6b17b647788bd419eb494fc079",
    "time": "2021-02-06T18:24:53.653Z",
    "type": "completion"
   },
   {
    "code": "chip_norm",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:26:53.230Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:26:53.399Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(2)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:27:09.279Z",
    "type": "execution"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:27:09.459Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:27:09.735Z",
    "type": "execution"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:27:09.879Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "idx": 5,
    "time": "2021-02-06T18:27:32.992Z",
    "type": "execution"
   },
   {
    "id": "81f3f938e2c94d988f9b7cd7027da05d",
    "time": "2021-02-06T18:27:33.107Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "idx": 6,
    "time": "2021-02-06T18:27:33.440Z",
    "type": "execution"
   },
   {
    "id": "d4504a9739224a928fc7397367dfcfe3",
    "time": "2021-02-06T18:27:33.576Z",
    "type": "completion"
   },
   {
    "code": "l1_result",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:27:44.399Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:27:44.499Z",
    "type": "completion"
   },
   {
    "code": "l1_result.GB",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:27:53.103Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:27:53.165Z",
    "type": "completion"
   },
   {
    "code": "l1_result.GB.unique()",
    "id": "42db734a766c45fba8b74a2a838c014d",
    "idx": 7,
    "time": "2021-02-06T18:27:57.411Z",
    "type": "execution"
   },
   {
    "id": "42db734a766c45fba8b74a2a838c014d",
    "time": "2021-02-06T18:27:57.481Z",
    "type": "completion"
   },
   {
    "code": "cnum_data[['#cnum']]",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:30:39.517Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:30:39.585Z",
    "type": "completion"
   },
   {
    "code": "cnum_data",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:30:49.443Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:30:49.514Z",
    "type": "completion"
   },
   {
    "code": "cnum_data.loc[:, ['L*', 'a*', 'b*']]",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:31:42.664Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:31:42.747Z",
    "type": "completion"
   },
   {
    "code": "cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:31:57.416Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:31:57.478Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = debug1['L*'] - \nnormalized_df=(debug1-debug1.mean())/debug1.std()",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:34:07.368Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:34:07.427Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = debug1['L*'] - \nnormalized_df=(debug1-debug1.mean())/debug1.std()\nnormalized_df",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:34:16.430Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:34:16.505Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean) / debug1['L*'].std()\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:35:43.131Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:35:43.245Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean) / debug1['L*'].std\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:00.211Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:00.325Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) #/ debug1['L*'].std\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:06.335Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:06.387Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:18.477Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:18.554Z",
    "type": "completion"
   },
   {
    "code": "debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\ndebug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\ndebug1",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:36:24.120Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:36:24.192Z",
    "type": "completion"
   },
   {
    "code": "# debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n# debug1",
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "idx": 4,
    "time": "2021-02-06T18:39:11.010Z",
    "type": "execution"
   },
   {
    "id": "bd9253b38b51423a9a3ea36ed78f65dd",
    "time": "2021-02-06T18:39:11.057Z",
    "type": "completion"
   },
   {
    "code": "locattions",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:39:58.590Z",
    "type": "execution"
   },
   {
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "time": "2021-02-06T18:39:58.665Z",
    "type": "completion"
   },
   {
    "code": "locations",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:40:00.807Z",
    "type": "execution"
   },
   {
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "time": "2021-02-06T18:40:00.869Z",
    "type": "completion"
   },
   {
    "code": "chip_num",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:40:06.496Z",
    "type": "execution"
   },
   {
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "time": "2021-02-06T18:40:06.563Z",
    "type": "completion"
   },
   {
    "code": "len(chip_num)",
    "id": "d8ce44b873434ba88decdb4f4709e46b",
    "idx": 6,
    "time": "2021-02-06T18:40:21.037Z",
    "type": "execution"
   },
   {
    "code": "l1",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 7,
    "time": "2021-02-06T18:44:08.553Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:44:08.624Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:45:07.997Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:45:08.264Z",
    "type": "completion"
   },
   {
    "code": "lab_norm",
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "idx": 6,
    "time": "2021-02-06T18:45:36.177Z",
    "type": "execution"
   },
   {
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "time": "2021-02-06T18:45:36.310Z",
    "type": "completion"
   },
   {
    "code": "lab_norm.shape",
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "idx": 6,
    "time": "2021-02-06T18:45:40.341Z",
    "type": "execution"
   },
   {
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "time": "2021-02-06T18:45:40.396Z",
    "type": "completion"
   },
   {
    "code": "len(lab_norm)",
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "idx": 6,
    "time": "2021-02-06T18:45:51.499Z",
    "type": "execution"
   },
   {
    "id": "4332d3385ba746e3b29bcac0b0c43061",
    "time": "2021-02-06T18:45:51.627Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest([[1, 1, 1]])\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:46:31.225Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:46:31.365Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest(torch.FloatTensor([[1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:47:37.740Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:47:37.815Z",
    "type": "completion"
   },
   {
    "code": "NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:48:01.822Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:48:01.883Z",
    "type": "completion"
   },
   {
    "code": "# #Neural Network Shape Test\n# NNtest = Neural_Network(inputSize = 3, outputSize = 1, hiddenSize = [3,3,3] , learning_rate = 0.001)\n# NNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:48:38.123Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:48:38.178Z",
    "type": "completion"
   },
   {
    "code": "unique_symbols",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:49:22.578Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:49:22.633Z",
    "type": "completion"
   },
   {
    "code": "l1",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:49:37.774Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:49:37.863Z",
    "type": "completion"
   },
   {
    "code": "l1['#Lnum']",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:50:46.716Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:50:46.795Z",
    "type": "completion"
   },
   {
    "code": "l1['#Lnum'].unique()",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:50:52.868Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:50:52.969Z",
    "type": "completion"
   },
   {
    "code": "unique_symbols",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:50:58.566Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:50:58.976Z",
    "type": "completion"
   },
   {
    "code": "l1_grouped",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:51:14.664Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:51:14.766Z",
    "type": "completion"
   },
   {
    "code": "l1_chip_abbrev_percentage",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:52:17.986Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:52:18.155Z",
    "type": "completion"
   },
   {
    "code": "len(l1_chip_abbrev_percentage)",
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "idx": 8,
    "time": "2021-02-06T18:52:31.119Z",
    "type": "execution"
   },
   {
    "id": "3e8b7c30c3154eeba55292a9c9f2a338",
    "time": "2021-02-06T18:52:31.188Z",
    "type": "completion"
   },
   {
    "code": "l1_result",
    "id": "1a980cbd8b2744578380cb27f9517068",
    "idx": 9,
    "time": "2021-02-06T18:52:53.003Z",
    "type": "execution"
   },
   {
    "id": "1a980cbd8b2744578380cb27f9517068",
    "time": "2021-02-06T18:52:53.092Z",
    "type": "completion"
   },
   {
    "code": "l1_result.shape",
    "id": "1a980cbd8b2744578380cb27f9517068",
    "idx": 9,
    "time": "2021-02-06T18:52:58.477Z",
    "type": "execution"
   },
   {
    "id": "1a980cbd8b2744578380cb27f9517068",
    "time": "2021-02-06T18:52:58.608Z",
    "type": "completion"
   },
   {
    "code": "chip_norm",
    "id": "1a980cbd8b2744578380cb27f9517068",
    "idx": 9,
    "time": "2021-02-06T18:53:26.083Z",
    "type": "execution"
   },
   {
    "id": "1a980cbd8b2744578380cb27f9517068",
    "time": "2021-02-06T18:53:26.329Z",
    "type": "completion"
   },
   {
    "code": "l1_chip_abbrev_percentage",
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "idx": 9,
    "time": "2021-02-06T18:53:37.681Z",
    "type": "execution"
   },
   {
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "time": "2021-02-06T18:53:37.886Z",
    "type": "completion"
   },
   {
    "code": "l1_chip_abbrev_percentage == chip_norm",
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "idx": 9,
    "time": "2021-02-06T18:54:01.573Z",
    "type": "execution"
   },
   {
    "id": "6a4ca79e1c1f4970821c477347770df2",
    "time": "2021-02-06T18:54:01.634Z",
    "type": "completion"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n                              for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "d87e693cad3e42ed8e6cc801b9ea65a0",
    "idx": 7,
    "time": "2021-02-06T18:55:04.151Z",
    "type": "execution"
   },
   {
    "id": "d87e693cad3e42ed8e6cc801b9ea65a0",
    "time": "2021-02-06T18:55:04.267Z",
    "type": "completion"
   },
   {
    "code": "range(1, 10)",
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "idx": 8,
    "time": "2021-02-06T18:55:50.684Z",
    "type": "execution"
   },
   {
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "time": "2021-02-06T18:55:50.738Z",
    "type": "completion"
   },
   {
    "code": "np.arange(1, 10)",
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "idx": 8,
    "time": "2021-02-06T18:55:56.492Z",
    "type": "execution"
   },
   {
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "time": "2021-02-06T18:55:56.559Z",
    "type": "completion"
   },
   {
    "code": "l1",
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "idx": 8,
    "time": "2021-02-06T18:56:08.844Z",
    "type": "execution"
   },
   {
    "id": "166fd4bb85d84a50b6e68cd2a14428f6",
    "time": "2021-02-06T18:56:08.948Z",
    "type": "completion"
   },
   {
    "code": "# #Neural Network Shape Test\nNNtest = Neural_Network(inputSize = 3, outputSize = 9, hiddenSize = [3,3,3] , learning_rate = 0.001)\nNNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "idx": 6,
    "time": "2021-02-06T18:57:33.980Z",
    "type": "execution"
   },
   {
    "id": "a0f7425e61a24e9287eedeb3847c96a9",
    "time": "2021-02-06T18:57:34.052Z",
    "type": "completion"
   },
   {
    "code": "lab_train",
    "id": "21db5e73be92486186d3fda30923ec0e",
    "idx": 10,
    "time": "2021-02-06T18:58:18.513Z",
    "type": "execution"
   },
   {
    "id": "21db5e73be92486186d3fda30923ec0e",
    "time": "2021-02-06T18:58:18.676Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 50\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [10], colors_num), (input_size, [50], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "edd0058b1003415baca0ba38b6e34e64",
    "idx": 9,
    "time": "2021-02-06T18:59:51.920Z",
    "type": "execution"
   },
   {
    "id": "edd0058b1003415baca0ba38b6e34e64",
    "time": "2021-02-06T18:59:52.152Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_test, NN(input_test))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "f6ce5e0006d345e19be7b61f8d00e067",
    "idx": 10,
    "time": "2021-02-06T18:59:52.444Z",
    "type": "execution"
   },
   {
    "id": "f6ce5e0006d345e19be7b61f8d00e067",
    "time": "2021-02-06T19:01:15.324Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:01:24.056Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:01:24.162Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:02:43.827Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:02:43.934Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    print(shape[0], shape[1], shape[2])\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:02:58.448Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:02:58.545Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    print(net-num)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:03:45.357Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:03:45.458Z",
    "type": "completion"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    print(net_num)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "idx": 11,
    "time": "2021-02-06T19:03:51.700Z",
    "type": "execution"
   },
   {
    "id": "7830aa56eb6d479488d2fc63bc3e0007",
    "time": "2021-02-06T19:03:51.777Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "64d8564a7b884a1984405f392fabb21e",
    "idx": 12,
    "time": "2021-02-06T19:07:47.762Z",
    "type": "execution"
   },
   {
    "id": "64d8564a7b884a1984405f392fabb21e",
    "time": "2021-02-06T19:07:47.842Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "d910e0e2e334411483adc407dcd04424",
    "idx": 13,
    "time": "2021-02-06T19:07:49.283Z",
    "type": "execution"
   },
   {
    "id": "d910e0e2e334411483adc407dcd04424",
    "time": "2021-02-06T19:07:49.330Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "818fef46e19e4960a9b9281a596a52f7",
    "idx": 14,
    "time": "2021-02-06T19:08:00.217Z",
    "type": "execution"
   },
   {
    "code": "import sys\nimport math\nimport numpy as np\n\"\"\"\nnot working\nfrom Language_Data_Scraper.py import *\n\"\"\"\nsys.path.insert(0, '..')\nfrom net_framework import *\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd",
    "id": "0e403ae3e9b4423785f11c3d7b46a36c",
    "idx": 0,
    "time": "2021-02-06T22:44:54.280Z",
    "type": "execution"
   },
   {
    "code": "term_data = pd.read_csv('term.txt', sep=\"\\t\", header=None)\nterm_data.columns = [\"#Lnum\", \"#snum\", \"#cnum\", \"Term Abbrev\"]\nterm_data.head()",
    "id": "49e9b39ffb2944d48b6ccdc09010b658",
    "idx": 1,
    "time": "2021-02-06T22:44:54.450Z",
    "type": "execution"
   },
   {
    "code": "term_data['#Lnum'].unique()",
    "id": "5121391a3fb9447c863f9b6a21ed2789",
    "idx": 2,
    "time": "2021-02-06T22:44:54.610Z",
    "type": "execution"
   },
   {
    "code": "cnum_data = pd.read_csv('cnum-vhcm-lab-new.txt', sep=\"\\t\")\nlocations = cnum_data[['#cnum']]\nlocations['Normalized-L'] = (cnum_data['L*'] - cnum_data['L*'].mean())/(cnum_data['L*'] - cnum_data['L*'].mean()).max()\nlocations['Normalized-a'] = (cnum_data['a*'] - cnum_data['a*'].mean())/(cnum_data['a*'] - cnum_data['a*'].mean()).std() * 1/2\nlocations['Normalized-b'] = (cnum_data['b*'] - cnum_data['b*'].mean())/(cnum_data['b*'] - cnum_data['b*'].mean()).std() * 1/2\ndisplay(locations.head(5))",
    "id": "03a37b7ff7ce43208b74d0bbc44eea00",
    "idx": 3,
    "time": "2021-02-06T22:44:54.767Z",
    "type": "execution"
   },
   {
    "code": "# debug1 = cnum_data.loc[:, ['#cnum', 'L*', 'a*', 'b*']]\n# debug1['L*'] = (debug1['L*'] - debug1['L*'].mean()) / debug1['L*'].std()\n# debug1",
    "id": "3b364c7fbfe047e990150f2894e5d207",
    "idx": 4,
    "time": "2021-02-06T22:44:54.922Z",
    "type": "execution"
   },
   {
    "code": "locations = locations.sort_values('#cnum')\nchip_num = list(locations['#cnum'])\nlab_norm = [[row[2], row[3], row[4]] for row in locations.itertuples()]",
    "id": "90feda1a5c5c41a68e68ac9822b47500",
    "idx": 5,
    "time": "2021-02-06T22:44:55.585Z",
    "type": "execution"
   },
   {
    "code": "# #Neural Network Shape Test\n# NNtest = Neural_Network(inputSize = 3, outputSize = 9, hiddenSize = [3,3,3] , learning_rate = 0.001)\n# NNtest(torch.FloatTensor([[1, 1, 1], [1, 1, 1]]))\n",
    "id": "c4306a3ea3554ad2a7b64ce9a660fc86",
    "idx": 6,
    "time": "2021-02-06T22:44:56.042Z",
    "type": "execution"
   },
   {
    "code": "l1 = term_data[term_data.get('#Lnum').eq(1)]\nunique_symbols = list(l1['Term Abbrev'].unique())\nl1_grouped = l1.groupby('#cnum')['Term Abbrev'].apply(list)\nl1_chip_abbrev_percentage = [[(l1_grouped[i + 1].count(abbrev) / len(l1_grouped[i + 1])) \\\n                              for abbrev in unique_symbols] for i in range(len(l1_grouped))]",
    "id": "70e03cca165945b78f53df7ac997e754",
    "idx": 7,
    "time": "2021-02-06T22:44:56.541Z",
    "type": "execution"
   },
   {
    "id": "0e403ae3e9b4423785f11c3d7b46a36c",
    "time": "2021-02-06T22:44:56.693Z",
    "type": "completion"
   },
   {
    "code": "l1_result = pd.DataFrame(l1_chip_abbrev_percentage)\nl1_result.index += 1\nl1_result.index.name = '#cnum'\nl1_result.columns = unique_symbols\nprint(l1_result)\nchip_norm = []\n#pull the percentage for each cnum\nfor x in chip_num:\n#     chip_norm.append((l1_result.loc[l1[\"#cnum\"]==x]).values.tolist()[0])\n    chip_norm.append(l1_result.loc[x,:].values.tolist())",
    "id": "fc21542794344e2781ed7c8323668401",
    "idx": 8,
    "time": "2021-02-06T22:44:56.814Z",
    "type": "execution"
   },
   {
    "id": "49e9b39ffb2944d48b6ccdc09010b658",
    "time": "2021-02-06T22:44:57.172Z",
    "type": "completion"
   },
   {
    "id": "5121391a3fb9447c863f9b6a21ed2789",
    "time": "2021-02-06T22:44:57.181Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 50\n#Size of training dataset\nnum_examples = 2000\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = [(input_size, [10], colors_num), (input_size, [50], colors_num), (input_size, [100], colors_num)]\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\nlab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2)\n\ninput_train = torch.FloatTensor(lab_train)\noutput_train = torch.FloatTensor(chip_train)\ninput_test= torch.FloatTensor(lab_test)\noutput_test = torch.FloatTensor(chip_test)",
    "id": "01693c63b3c54f879d46abdd5696ea35",
    "idx": 9,
    "time": "2021-02-06T22:44:57.196Z",
    "type": "execution"
   },
   {
    "id": "03a37b7ff7ce43208b74d0bbc44eea00",
    "time": "2021-02-06T22:44:57.225Z",
    "type": "completion"
   },
   {
    "id": "3b364c7fbfe047e990150f2894e5d207",
    "time": "2021-02-06T22:44:57.241Z",
    "type": "completion"
   },
   {
    "id": "90feda1a5c5c41a68e68ac9822b47500",
    "time": "2021-02-06T22:44:57.257Z",
    "type": "completion"
   },
   {
    "id": "c4306a3ea3554ad2a7b64ce9a660fc86",
    "time": "2021-02-06T22:44:57.272Z",
    "type": "completion"
   },
   {
    "id": "70e03cca165945b78f53df7ac997e754",
    "time": "2021-02-06T22:44:57.373Z",
    "type": "completion"
   },
   {
    "id": "fc21542794344e2781ed7c8323668401",
    "time": "2021-02-06T22:44:57.444Z",
    "type": "completion"
   },
   {
    "id": "01693c63b3c54f879d46abdd5696ea35",
    "time": "2021-02-06T22:44:57.445Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\nfor net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n    for i in range(num_iters):\n        #Calculating l1 error\n        error = NN.l1error(output_test, NN(input_test))\n        if i == 0: \n            dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n        else:\n            dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n        NN.train(input_train, output_train)\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "04811196407b40048e92e59766e727ff",
    "idx": 10,
    "time": "2021-02-06T22:44:57.481Z",
    "type": "execution"
   },
   {
    "code": "for net_num, shape in enumerate(network_shapes):\n    print('Network Shape:',flush = True)\n    print('Input Size = ' + str(shape[0]), flush = True)\n    print('Hidden Size = ' + str(shape[1]), flush = True)\n    print('Output Size = ' + str(shape[2]), flush = True)\n    # Loading the network we trained in the prev. section\n    \n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    print(net_num)\n    NN.load_state_dict(torch.load(\"saved_networks/Net \" + str(net_num)))\n    \n    validation_error = NN.l1error(output_test, NN(input_test))\n    max_error = np.max(np.abs((output_test - NN(input_test)).detach().numpy()))\n    print(\"The validation error is: \" + str(validation_error), flush = True)  \n    print(\"The maximum error is:\" + str(max_error), flush = True)",
    "id": "5fcbbc9de1f3451b8c81b54e170f3d99",
    "idx": 11,
    "time": "2021-02-06T22:44:58.224Z",
    "type": "execution"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "2d09069ca66742728f94f15c244e85cf",
    "idx": 12,
    "time": "2021-02-06T22:44:59.542Z",
    "type": "execution"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "idx": 13,
    "time": "2021-02-06T22:44:59.786Z",
    "type": "execution"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "81206a89a4654a4cb6e7e785c66e9a56",
    "idx": 14,
    "time": "2021-02-06T22:45:00.167Z",
    "type": "execution"
   },
   {
    "id": "04811196407b40048e92e59766e727ff",
    "time": "2021-02-06T22:45:51.868Z",
    "type": "completion"
   },
   {
    "id": "5fcbbc9de1f3451b8c81b54e170f3d99",
    "time": "2021-02-06T22:45:51.869Z",
    "type": "completion"
   },
   {
    "id": "2d09069ca66742728f94f15c244e85cf",
    "time": "2021-02-06T22:45:51.870Z",
    "type": "completion"
   },
   {
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "time": "2021-02-06T22:45:51.871Z",
    "type": "completion"
   },
   {
    "id": "81206a89a4654a4cb6e7e785c66e9a56",
    "time": "2021-02-06T22:45:51.875Z",
    "type": "completion"
   },
   {
    "code": "node_num = range(1,25)\nlayer_num = range(1,4)\n\n\nshape_collection = []\n\ndef trickle(arr, iteration_left, check):\n    if iteration_left == 0:\n        global shape_collection\n        #running the int fxn to make sure we don't have floats\n        mp = map(int, arr)\n        x = list(mp)\n        if check == sum(x):\n            shape_collection.append(x)\n    else:\n        new_arr = [0]+ arr + [0]\n        #recursively expanding the list symmetrically\n        while new_arr[0] < new_arr[1]-2 and new_arr[-1] < new_arr[-2]-2:\n            new_arr[0] += 1\n            new_arr[1] -= 1\n            new_arr[-1] += 1\n            new_arr[-2] -= 1\n        trickle(new_arr, iteration_left - 1, check)\n\nfor node in node_num:\n    for layer in layer_num:\n        if node//layer < 3:\n            continue\n        if layer%2 == 0:\n            trickle([node/2, node/2], (layer-2)/2, node)\n        else:\n            trickle([node], (layer-1)/2, node)\n        \n        \n        \n        \n\nprint(shape_collection)",
    "id": "2d09069ca66742728f94f15c244e85cf",
    "idx": 12,
    "time": "2021-02-06T22:46:01.870Z",
    "type": "execution"
   },
   {
    "id": "2d09069ca66742728f94f15c244e85cf",
    "time": "2021-02-06T22:46:01.938Z",
    "type": "completion"
   },
   {
    "code": "#Number of training iterations\nnum_iters = 500\n\n#Listing out the shapes of each model\ncolors_num = len(chip_norm[0])\ninput_size = 3\n\nnetwork_shapes = []\nfor s in shape_collection:\n    network_shapes.append((input_size,s,colors_num))\n\n#Learning rate of the network\nrate = 0.001\n\n#Generating Training Data\ninput_train, output_train, input_test, output_test = 0, 0, 0, 0\ndef shuffle():\n    lab_train, lab_test, chip_train, chip_test = train_test_split(lab_norm, chip_norm, test_size=0.2, random_state = 3, shuffle = True)\n    #test run on whole dataset\n    #lab_train, lab_test = lab_norm, lab_norm\n    #chip_train, chip_test = chip_norm, chip_norm\n    \n    global input_train, output_train, input_test, output_test\n    input_train = torch.FloatTensor(lab_train)\n    output_train = torch.FloatTensor(chip_train)\n    input_test= torch.FloatTensor(lab_test)\n    output_test = torch.FloatTensor(chip_test)\n\nprint(network_shapes)",
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "idx": 13,
    "time": "2021-02-06T22:46:02.509Z",
    "type": "execution"
   },
   {
    "id": "0fe8dc7016284b4f8380932c7b98392f",
    "time": "2021-02-06T22:46:02.561Z",
    "type": "completion"
   },
   {
    "code": "#Array of losses over training period for each network\noutput_file = {}\nfor n in node_num:\n    output_file[n] = {}\n    \nfor net_num, shape in enumerate(network_shapes):\n    shuffle()\n    print(\"Training: \",shape)\n    NN = Neural_Network(inputSize = shape[0], outputSize = shape[2],\n                        hiddenSize = shape[1] , learning_rate = rate)\n    error_arr = []\n    prev_error = 0\n    strike = 0\n    \n    for i in range(num_iters):       \n        NN.train(input_train, output_train)\n        validation_error = NN.l1error(output_test, NN(input_test))\n        #zero mistake counter at new training\n        if i == 0:\n            strike = 0\n        #adding error to array\n        error_arr.append(validation_error)\n        #wait for them to grow up\n        if prev_error < validation_error and i > 100:\n            if strike > 3:\n                print(\"Complete at iteration \", i, \"\\nFinal error: \", validation_error, \"\\n\")\n                break\n            else:\n                strike += 1\n        prev_error = validation_error\n\n    #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n    NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num))\n    output_file[sum(shape[1])][len(shape[1])] = error_arr",
    "id": "81206a89a4654a4cb6e7e785c66e9a56",
    "idx": 14,
    "time": "2021-02-06T22:46:03.018Z",
    "type": "execution"
   },
   {
    "code": "# #Array of losses over training period for each network\n# for net_num, shape in enumerate(network_shapes):\n#     print('Network Shape:',flush = True)\n#     print('Input Size = ' + str(shape[0]), flush = True)\n#     print('Hidden Size = ' + str(shape[1]), flush = True)\n#     print('Output Size = ' + str(shape[2]), flush = True)\n#     NN = Neural_Network(inputSize = 3, outputSize = colors_num, hiddenSize = [3,3,3] , learning_rate = 0.001)\n\n#     for i in range(num_iters):\n#         #Calculating l1 error\n#         error = NN.l1error(output_test, NN(input_test))\n#         if i == 0: \n#             dh = display(\"#\" + str(i) + \" Error: \" + str(error), display_id=True)\n#         else:\n#             dh.update(\"#\" + str(i) + \" Error: \" + str(error))\n            \n#         NN.train(input_train, output_train)\n#     #Saves the training results in a filed named \"Net 0\", \"Net 1\", etc. \n#     NN.saveWeights(model = NN, path = \"saved_networks/Net \" + str(net_num));",
    "id": "04811196407b40048e92e59766e727ff",
    "idx": 10,
    "time": "2021-02-06T22:46:39.071Z",
    "type": "execution"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "277px",
    "left": "1061px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
